{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d725f7c-b9f8-4f22-9617-20eb8fde5ee0",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd73f61-5d97-48d3-84c6-801543127272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03e8a96-ce65-4118-a26e-10e4ef9a9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로부터 직접 생성하기\n",
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fdc157c-3c8d-45d8-a76b-2f5bf7b485b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy 배열로부터 생성하기\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b1af894-7604-40d0-af7f-6b4c99431f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.2575, 0.7198],\n",
      "        [0.0221, 0.5490]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다른 텐서로부터 생성하기\n",
    "# 명시적으로 override하지 않는다면, 인자로 주어진 텐서의 속성(shape, datatype)을 유지함\n",
    "x_ones = torch.ones_like(x_data) # x_data의 속성을 유지함\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # x_data의 속성을 덮어씀\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42a7a31d-dc22-4a9d-ab4d-8b3667b9e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.1917, 0.6112, 0.0914],\n",
      "        [0.5819, 0.9631, 0.4271]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random or constant 값 사용하기\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbfbfc7-daee-492f-98a1-d1f0e1f5c322",
   "metadata": {},
   "source": [
    "# Tensor Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af2ff0b9-3225-4f69-911d-9c1d8e61c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of tensor: torch.Size([3, 4])\n",
      "datatype of tensor: torch.float32\n",
      "device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# 텐서의 속성은 텐서의 shape, datatype 및 어느 장치에 저장되는지를 나타냄\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"shape of tensor: {tensor.shape}\")\n",
    "print(f\"datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b651b-a67b-466c-afd1-4f7c68b41263",
   "metadata": {},
   "source": [
    "# Tensor Operation\n",
    "\n",
    "전치(transposing), 인덱싱(indexing), 슬라이싱(slicing), 수학 계산, 선형 대수, 임의 샘플링 등\n",
    "[tensor 연산들](https://pytorch.org/docs/stable/torch.html)\n",
    "\n",
    "각 연산들은 (일반적으로 CPU보다 빠른) GPU에서 실행 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29af1a5f-0ad9-41aa-88e0-bccd7df5035a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda isn't available\n"
     ]
    }
   ],
   "source": [
    "# GPU가 존재하면 텐서를 이동\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    print(f\"device tensor is stored on: {tensor.device}\")\n",
    "else:\n",
    "    print(\"cuda isn't available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8dcf037-b83b-46a5-8ed8-867ff8ded6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# NumPy식의 표준 인덱싱과 슬라이싱\n",
    "tensor = torch.ones(4,4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efc1f962-7429-47b1-a47e-c1057547720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "tensor([[[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 합치기 torch.cat을 사용하여 주어진 차원에 따라 일련의 텐서를 연결할 수 있음\n",
    "# tensor.cat\n",
    "# 여러 텐서를 지정된 차원에서 연결\n",
    "# 기존 차원을 따라 이어붙이는 방식 -> 차원 수가 변하지 않음\n",
    "# 텐서들의 크기가 연결할 차원을 제외하고 동일해야 함\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "\n",
    "# tensor.stack\n",
    "# 여러 텐서를 새로운 차원을 추가하여 결합함\n",
    "# 기존 텐서의 차원보다 1차원이 증가함\n",
    "# torch.stack을 사용하려면 모든 텐서의 크기가 완전히 동일해야 함\n",
    "t2 = torch.stack([tensor, tensor, tensor], dim=1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a87e40e5-492e-405a-afd0-5c0869b77714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor) \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor \n",
      " tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 텐서 곱하기\n",
    "# 요소별 곱(element-wise product)을 계산함\n",
    "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
    "# 다른 문법\n",
    "print(f\"tensor * tensor \\n {tensor * tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2347eddd-3ef7-4ac6-8362-242f1df56e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T) \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T \n",
      " tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 두 텐서 간의 행렬 곱(matrix multiplication)을 계산합니다\n",
    "print(f\"tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n\")\n",
    "# 다른 문법\n",
    "print(f\"tensor @ tensor.T \\n {tensor @ tensor.T}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6b33757-b751-4337-9038-ace8f5a64fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "# 바꿔치기(in-place) 연산: 접미사를 갖는 연산들은 in-place 연산임 (ex: x.copy(), x.t()는 x를 변경)\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c699a5-4e0b-4c9a-a240-97c95bd176c5",
   "metadata": {},
   "source": [
    "# NumPy 변환 (Bridge)\n",
    "CPU상의 텐서와 NumPy 배열은 메모리 공간을 공유하기 때문에, 하나를 변경하면 다른 하나도 변경됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6e745-2581-4afe-beb0-e3d5ed926b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서를 NumPy 배열로 변환하기\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
